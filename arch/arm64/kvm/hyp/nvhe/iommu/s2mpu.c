// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2021 - Google LLC
 * Author: David Brazdil <dbrazdil@google.com>
 */

#include <linux/kvm_host.h>

#include <asm/kvm_asm.h>
#include <asm/kvm_hyp.h>
#include <asm/kvm_mmu.h>
#include <asm/kvm_s2mpu.h>

#include <linux/arm-smccc.h>

#include <nvhe/iommu.h>
#include <nvhe/memory.h>
#include <nvhe/mm.h>
#include <nvhe/spinlock.h>
#include <nvhe/trap_handler.h>

#define SMC_CMD_PREPARE_PD_ONOFF	0x82000410
#define SMC_MODE_POWER_UP		1

#define PA_MAX				((phys_addr_t)SZ_1G * NR_GIGABYTES)

#define for_each_s2mpu(i) \
	for ((i) = &s2mpus[0]; (i) != &s2mpus[nr_s2mpus]; (i)++)

#define for_each_powered_s2mpu(i) \
	for_each_s2mpu((i)) if (is_powered_on((i)))

#define CTX_CFG_ENTRY(ctxid, nr_ctx, vid) \
	(CONTEXT_CFG_VALID_VID_CTX_VID(ctxid, vid) \
	 | (((ctxid) < (nr_ctx)) ? CONTEXT_CFG_VALID_VID_CTX_VALID(ctxid) : 0))

static size_t __ro_after_init			nr_s2mpus;
static struct pkvm_iommu __ro_after_init	*s2mpus;
static struct mpt				host_mpt;
static hyp_spinlock_t				s2mpu_lock;

struct s2mpu_drv_data {
	u32 version;
	u32 context_cfg_valid_vid;
};

static bool is_version(struct pkvm_iommu *dev, u32 version)
{
	struct s2mpu_drv_data *data = (struct s2mpu_drv_data *)dev->data;

	return (data->version & VERSION_CHECK_MASK) == version;
}

static bool is_powered_on(struct pkvm_iommu *dev)
{
	return dev->powered;
}

static bool is_in_power_domain(struct pkvm_iommu *dev, u64 power_domain_id)
{
	return false;
}

static u32 __context_cfg_valid_vid(struct pkvm_iommu *dev, u32 vid_bmap)
{
	struct s2mpu_drv_data *data = (struct s2mpu_drv_data *)dev->data;
	u8 ctx_vid[NR_CTX_IDS] = { 0 };
	unsigned int vid, ctx = 0;
	unsigned int num_ctx;
	u32 res;

	/* Only initialize once. */
	if (data->context_cfg_valid_vid)
		return data->context_cfg_valid_vid;

	num_ctx = readl_relaxed(dev->va + REG_NS_NUM_CONTEXT) & NUM_CONTEXT_MASK;
	while (vid_bmap) {
		/* Break if we cannot allocate more. */
		if (ctx >= num_ctx)
			break;

		vid = __ffs(vid_bmap);
		vid_bmap &= ~BIT(vid);
		ctx_vid[ctx++] = vid;
	}

	/* The following loop was unrolled so bitmasks are constant. */
	BUILD_BUG_ON(NR_CTX_IDS != 8);
	res = CTX_CFG_ENTRY(0, ctx, ctx_vid[0])
	    | CTX_CFG_ENTRY(1, ctx, ctx_vid[1])
	    | CTX_CFG_ENTRY(2, ctx, ctx_vid[2])
	    | CTX_CFG_ENTRY(3, ctx, ctx_vid[3])
	    | CTX_CFG_ENTRY(4, ctx, ctx_vid[4])
	    | CTX_CFG_ENTRY(5, ctx, ctx_vid[5])
	    | CTX_CFG_ENTRY(6, ctx, ctx_vid[6])
	    | CTX_CFG_ENTRY(7, ctx, ctx_vid[7]);

	data->context_cfg_valid_vid = res;
	return res;
}

static int __initialize_v9(struct pkvm_iommu *dev)
{
	u32 ssmt_valid_vid_bmap, ctx_cfg;

	/* Assume all VIDs may be generated by the connected SSMTs for now. */
	ssmt_valid_vid_bmap = ALL_VIDS_BITMAP;
	ctx_cfg = __context_cfg_valid_vid(dev, ssmt_valid_vid_bmap);
	if (!ctx_cfg)
		return -EINVAL;

	/*
	 * Write CONTEXT_CFG_VALID_VID configuration before touching L1ENTRY*
	 * registers. Writes to those registers are ignored unless there is
	 * a context ID allocated to the corresponding VID (v9 only).
	 */
	writel_relaxed(ctx_cfg, dev->va + REG_NS_CONTEXT_CFG_VALID_VID);
	return 0;
}

static int __initialize(struct pkvm_iommu *dev)
{
	struct s2mpu_drv_data *data = (struct s2mpu_drv_data *)dev->data;

	if (!data->version)
		data->version = readl_relaxed(dev->va + REG_NS_VERSION);

	switch (data->version & VERSION_CHECK_MASK) {
	case S2MPU_VERSION_8:
		return 0;
	case S2MPU_VERSION_9:
		return __initialize_v9(dev);
	default:
		return -EINVAL;
	}
}

static void __set_control_regs(struct pkvm_iommu *dev)
{
	u32 ctrl0 = 0, irq_vids;

	/*
	 * Note: We set the values of CTRL0, CTRL1 and CFG registers here but we
	 * still rely on the correctness of their reset values. S2MPUs *must*
	 * reset to a state where all DMA traffic is blocked until the hypervisor
	 * writes its configuration to the S2MPU. A malicious EL1 could otherwise
	 * attempt to bypass the permission checks in the window between powering
	 * on the S2MPU and this function being called.
	 */

	/* Enable the S2MPU, otherwise all traffic would be allowed through. */
	ctrl0 |= CTRL0_ENABLE;

	/*
	 * Enable interrupts on fault for all VIDs. The IRQ must also be
	 * specified in DT to get unmasked in the GIC.
	 */
	ctrl0 |= CTRL0_INTERRUPT_ENABLE;
	irq_vids = ALL_VIDS_BITMAP;

	/* Return SLVERR/DECERR to device on permission fault. */
	ctrl0 |= is_version(dev, S2MPU_VERSION_9) ? CTRL0_FAULT_RESP_TYPE_DECERR
						  : CTRL0_FAULT_RESP_TYPE_SLVERR;

	writel_relaxed(irq_vids, dev->va + REG_NS_INTERRUPT_ENABLE_PER_VID_SET);
	writel_relaxed(0, dev->va + REG_NS_CFG);
	writel_relaxed(0, dev->va + REG_NS_CTRL1);
	writel_relaxed(ctrl0, dev->va + REG_NS_CTRL0);
}

/* Poll the given SFR as long as its value has all bits of a given mask set. */
static void __wait_while(void __iomem *addr, u32 mask)
{
	while ((readl_relaxed(addr) & mask) == mask)
		continue;
}

static void __wait_for_invalidation_complete(struct pkvm_iommu *dev)
{
	/* Must not access SFRs while S2MPU is busy invalidating (v9 only). */
	if (is_version(dev, S2MPU_VERSION_9)) {
		__wait_while(dev->va + REG_NS_STATUS,
			     STATUS_BUSY | STATUS_ON_INVALIDATING);
	}
}

static void __all_invalidation(struct pkvm_iommu *dev)
{
	writel_relaxed(INVALIDATION_INVALIDATE, dev->va + REG_NS_ALL_INVALIDATION);
	__wait_for_invalidation_complete(dev);
}

static void __range_invalidation(struct pkvm_iommu *dev, phys_addr_t first_byte,
				 phys_addr_t last_byte)
{
	u32 start_ppn = first_byte >> RANGE_INVALIDATION_PPN_SHIFT;
	u32 end_ppn = last_byte >> RANGE_INVALIDATION_PPN_SHIFT;

	writel_relaxed(start_ppn, dev->va + REG_NS_RANGE_INVALIDATION_START_PPN);
	writel_relaxed(end_ppn, dev->va + REG_NS_RANGE_INVALIDATION_END_PPN);
	writel_relaxed(INVALIDATION_INVALIDATE, dev->va + REG_NS_RANGE_INVALIDATION);
	__wait_for_invalidation_complete(dev);
}

static void __set_l1entry_attr_with_prot(struct pkvm_iommu *dev, unsigned int gb,
					 unsigned int vid, enum mpt_prot prot)
{
	writel_relaxed(L1ENTRY_ATTR_1G(prot),
		       dev->va + REG_NS_L1ENTRY_ATTR(vid, gb));
}

static void __set_l1entry_attr_with_fmpt(struct pkvm_iommu *dev, unsigned int gb,
					 unsigned int vid, struct fmpt *fmpt)
{
	if (fmpt->gran_1g) {
		__set_l1entry_attr_with_prot(dev, gb, vid, fmpt->prot);
	} else {
		/* Order against writes to the SMPT. */
		writel(L1ENTRY_ATTR_L2(SMPT_GRAN_ATTR),
		       dev->va + REG_NS_L1ENTRY_ATTR(vid, gb));
	}
}

static void __set_l1entry_l2table_addr(struct pkvm_iommu *dev, unsigned int gb,
				       unsigned int vid, phys_addr_t addr)
{
	/* Order against writes to the SMPT. */
	writel(L1ENTRY_L2TABLE_ADDR(addr),
	       dev->va + REG_NS_L1ENTRY_L2TABLE_ADDR(vid, gb));
}

/*
 * Initialize S2MPU device and set all GB regions to 1G granularity with
 * given protection bits.
 */
static int initialize_with_prot(struct pkvm_iommu *dev, enum mpt_prot prot)
{
	unsigned int gb, vid;
	int ret;

	ret = __initialize(dev);
	if (ret)
		return ret;

	for_each_gb_and_vid(gb, vid)
		__set_l1entry_attr_with_prot(dev, gb, vid, prot);
	__all_invalidation(dev);

	/* Set control registers, enable the S2MPU. */
	__set_control_regs(dev);
	return 0;
}

/*
 * Initialize S2MPU device, set L2 table addresses and configure L1TABLE_ATTR
 * registers according to the given MPT struct.
 */
static int initialize_with_mpt(struct pkvm_iommu *dev, struct mpt *mpt)
{
	unsigned int gb, vid;
	struct fmpt *fmpt;
	int ret;

	ret = __initialize(dev);
	if (ret)
		return ret;

	for_each_gb_and_vid(gb, vid) {
		fmpt = &mpt->fmpt[gb];
		__set_l1entry_l2table_addr(dev, gb, vid, __hyp_pa(fmpt->smpt));
		__set_l1entry_attr_with_fmpt(dev, gb, vid, fmpt);
	}
	__all_invalidation(dev);

	/* Set control registers, enable the S2MPU. */
	__set_control_regs(dev);
	return 0;
}

/*
 * Set MPT protection bits set to 'prot' in the give byte range (page-aligned).
 * Update currently powered S2MPUs.
 */
static void set_mpt_range_locked(struct mpt *mpt, phys_addr_t first_byte,
				 phys_addr_t last_byte, enum mpt_prot prot)
{
	unsigned int first_gb = first_byte / SZ_1G;
	unsigned int last_gb = last_byte / SZ_1G;
	size_t start_gb_byte, end_gb_byte;
	unsigned int gb, vid;
	struct pkvm_iommu *dev;
	struct fmpt *fmpt;
	enum mpt_update_flags flags;

	for_each_gb_in_range(gb, first_gb, last_gb) {
		fmpt = &mpt->fmpt[gb];
		start_gb_byte = (gb == first_gb) ? first_byte % SZ_1G : 0;
		end_gb_byte = (gb == last_gb) ? (last_byte % SZ_1G) + 1 : SZ_1G;

		flags = __set_fmpt_range(fmpt, start_gb_byte, end_gb_byte, prot);

		if (flags & MPT_UPDATE_L2)
			kvm_flush_dcache_to_poc(fmpt->smpt, SMPT_SIZE);

		if (flags & MPT_UPDATE_L1) {
			for_each_powered_s2mpu(dev) {
				for_each_vid(vid)
					__set_l1entry_attr_with_fmpt(dev, gb, vid, fmpt);
			}
		}
	}

	/* Invalidate range in all powered S2MPUs. */
	for_each_powered_s2mpu(dev)
		__range_invalidation(dev, first_byte, last_byte);
}

static void s2mpu_host_stage2_set_owner(phys_addr_t addr, size_t size,
					enum pkvm_component_id owner_id)
{
	/* Grant access only to the default owner of the page table (ID=0). */
	enum mpt_prot prot = owner_id ? MPT_PROT_NONE : MPT_PROT_RW;

	/*
	 * NOTE: The following code refers to 'end' as the exclusive upper
	 * bound and 'last' as the inclusive one.
	 */

	/*
	 * Sanitize inputs with S2MPU-specific physical address space bounds.
	 * Ownership change requests outside this boundary will be ignored.
	 * The S2MPU also specifies that the PA region 4-34GB always maps to
	 * PROT_NONE and the corresponding MMIO registers are read-only.
	 * Ownership changes in this region will have no effect.
	 */

	if (addr >= PA_MAX)
		return;

	size = min(size, (size_t)(PA_MAX - addr));
	if (size == 0)
		return;

	hyp_spin_lock(&s2mpu_lock);
	set_mpt_range_locked(&host_mpt,
			     ALIGN_DOWN(addr, SMPT_GRAN),
			     ALIGN(addr + size, SMPT_GRAN) - 1,
			     prot);
	hyp_spin_unlock(&s2mpu_lock);
}

static bool s2mpu_host_smc_handler(struct kvm_cpu_context *host_ctxt)
{
	DECLARE_REG(u64, fn, host_ctxt, 0);
	DECLARE_REG(u64, mode, host_ctxt, 1);
	DECLARE_REG(u64, domain_id, host_ctxt, 2);
	DECLARE_REG(u64, group, host_ctxt, 3);

	struct arm_smccc_res res;
	struct pkvm_iommu *dev;
	int ret;

	if (fn != SMC_CMD_PREPARE_PD_ONOFF)
		return false; /* SMC not handled */

	/*
	 * Host is notifying EL3 that a power domain was turned on/off.
	 * Use this SMC as a trigger to program the S2MPUs.
	 * Note that the host may be malicious and issue this SMC arbitrarily.
	 *
	 * Power on:
	 * It is paramount that the S2MPU reset state is enabled and blocking
	 * all traffic. That way the host is forced to issue a power-on SMC to
	 * unblock the S2MPUs.
	 *
	 * Power down:
	 * A power-down SMC is a hint for hyp to stop updating the S2MPU, lest
	 * writes to powered-down MMIO registers produce SErrors in the host.
	 * However, hyp must perform one last update - putting the S2MPUs back
	 * to their blocking reset state - in case the host does not actually
	 * power them down and continues issuing DMA traffic.
	 */

	hyp_spin_lock(&s2mpu_lock);
	arm_smccc_1_1_smc(fn, mode, domain_id, group, &res);
	ret = res.a0;

	if (ret == SMCCC_RET_SUCCESS) {
		for_each_s2mpu(dev) {
			if (!is_in_power_domain(dev, domain_id))
				continue;

			if (mode == SMC_MODE_POWER_UP) {
				dev->powered = true;
				ret = initialize_with_mpt(dev, &host_mpt);
			} else {
				ret = initialize_with_prot(dev, MPT_PROT_NONE);
				dev->powered = false;
			}
		}
	}
	hyp_spin_unlock(&s2mpu_lock);

	cpu_reg(host_ctxt, 0) = ret;
	return true;  /* SMC handled */
}

static struct pkvm_iommu *find_s2mpu_by_addr(phys_addr_t addr)
{
	struct pkvm_iommu *dev;

	for_each_s2mpu(dev) {
		if (dev->pa <= addr && addr < (dev->pa + S2MPU_MMIO_SIZE))
			return dev;
	}
	return NULL;
}

static u32 host_mmio_reg_access_mask(size_t off, bool is_write)
{
	const u32 no_access  = 0;
	const u32 read_write = (u32)(-1);
	const u32 read_only  = is_write ? no_access  : read_write;
	const u32 write_only = is_write ? read_write : no_access;
	u32 masked_off;

	/* IRQ handler can clear interrupts. */
	if (off == REG_NS_INTERRUPT_CLEAR)
		return write_only & ALL_VIDS_BITMAP;

	/* IRQ handler can read bitmap of pending interrupts. */
	if (off == REG_NS_FAULT_STATUS)
		return read_only & ALL_VIDS_BITMAP;

	/* IRQ handler can read fault information. */
	masked_off = off & ~REG_NS_FAULT_VID_MASK;
	if ((masked_off == REG_NS_FAULT_PA_LOW(0)) ||
	    (masked_off == REG_NS_FAULT_PA_HIGH(0)) ||
	    (masked_off == REG_NS_FAULT_INFO(0)))
		return read_only;

	return no_access;
}

static bool s2mpu_host_mmio_dabt_handler(struct kvm_cpu_context *host_ctxt,
					 phys_addr_t fault_pa, unsigned int len,
					 bool is_write, int rd)
{
	struct pkvm_iommu *dev;
	size_t off;
	u32 mask;

	/* Only handle MMIO access with u32 size and alignment. */
	if ((len != sizeof(u32)) || (fault_pa & (sizeof(u32) - 1)))
		return false;

	dev = find_s2mpu_by_addr(fault_pa);
	if (!dev || !is_powered_on(dev))
		return false;

	off = fault_pa - dev->pa;
	mask = host_mmio_reg_access_mask(off, is_write);
	if (!mask)
		return false;

	if (is_write)
		writel_relaxed(cpu_reg(host_ctxt, rd) & mask, dev->va + off);
	else
		cpu_reg(host_ctxt, rd) = readl_relaxed(dev->va + off) & mask;
	return true;
}

static int s2mpu_init(void *data, size_t size)
{
	struct mpt in_mpt;
	u32 *smpt;
	phys_addr_t pa;
	unsigned int gb;
	int ret = 0;

	if (size != sizeof(in_mpt))
		return -EINVAL;

	/* The host can concurrently modify 'data'. Copy it to avoid TOCTOU. */
	memcpy(&in_mpt, data, sizeof(in_mpt));

	/* Take ownership of all SMPT buffers. This will also map them in. */
	for_each_gb(gb) {
		smpt = kern_hyp_va(in_mpt.fmpt[gb].smpt);
		pa = __hyp_pa(smpt);

		if (!IS_ALIGNED(pa, SMPT_SIZE)) {
			ret = -EINVAL;
			break;
		}

		ret = __pkvm_host_donate_hyp(pa >> PAGE_SHIFT, SMPT_NUM_PAGES);
		if (ret)
			break;

		host_mpt.fmpt[gb] = (struct fmpt){
			.smpt = smpt,
			.gran_1g = true,
			.prot = MPT_PROT_NONE,
		};
	}

	/* Try to return memory back if there was an error. */
	if (ret) {
		for_each_gb(gb) {
			smpt = host_mpt.fmpt[gb].smpt;
			if (!smpt)
				break;

			WARN_ON(__pkvm_hyp_donate_host(__hyp_pa(smpt) >> PAGE_SHIFT,
						       SMPT_NUM_PAGES));
		}
		memset(&host_mpt, 0, sizeof(host_mpt));
	}

	return ret;
}

static int s2mpu_validate(phys_addr_t pa, size_t size)
{
	if (size != S2MPU_MMIO_SIZE)
		return -EINVAL;

	return 0;
}

const struct pkvm_iommu_ops pkvm_s2mpu_ops = (struct pkvm_iommu_ops){
	.init = s2mpu_init,
	.validate = s2mpu_validate,
	.data_size = sizeof(struct s2mpu_drv_data),
};

const struct kvm_iommu_ops kvm_s2mpu_ops = (struct kvm_iommu_ops){
	.host_smc_handler = s2mpu_host_smc_handler,
	.host_mmio_dabt_handler = s2mpu_host_mmio_dabt_handler,
	.host_stage2_set_owner = s2mpu_host_stage2_set_owner,
};
